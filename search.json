[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "",
    "text": "Overview\nSetting up a computer for running bioinformatic analysis can be a challenging process. Most bioinformatic applications involve the use of many different software packages, which are often part of long data processing pipelines. In this course we will teach you how to overcome these challenges by using two types of tools: package managers and workflow management software. You will learn about common tools to install software, with a particular focus on the popular Conda/Mamba package manager. We will also cover containerisation systems (Docker and Singularity), which are a way to further abstract software requirements by bundling them into virtual environments. Finally, we will teach you how to use automated pipelines to streamline your bioinformatic analysis. We will focus on the Nextflow software, introducing you to its core pipelines and how you can configure it to run at scale on HPC clusters.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nRecognise the use of package managers in bioinformatics.\nUse package managers to create and maintain complex software environments.\nUnderstand how containerisation solutions can be used to solve the problem of software dependencies.\nInstall, configure and run automated analysis pipelines developed and maintained by the bioinformatics community.\n\n\n\n\nTarget Audience\nThis course is aimed at researchers who are just starting to run bioinformatics analysis on their own. It may be particularly useful if you attended previous training on specific applications (e.g. RNA-seq, ChIP-seq, variant calling, etc.), but are struggling on how to setup and start your analysis. It is also useful if you are using a HPC cluster and would like to learn how to manage software and automate and parallelise your analysis using available pipelines.\nNote that the focus of this course is to introduce these tools from a hands-on and practical user perspective, not as a developer. Therefore, we will not teach you how to write your own pipelines, or create your own software containers or installation recipes.\nWe will also not cover the details of any specific type of bioinformatic analysis. The idea of this course is to introduce the computational tools to get your work done, not to teach how those tools work.\n\n\nPrerequisites\n\nUnix command line (required): you should be comfortable using the command line to navigate your filesystem and understand the basic structure of a command.\nNGS data analysis (desirable): if you are familiar with the basic analysis of a specific type of NGS data (e.g. RNA-seq, ChIP-seq, WGS), it will help you to engage with some of the examples used in the course. However, you can attend this course even if you haven’t done any of those analysis before.\n\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nRaquel Manzano Garcia  \nAffiliation: Cancer Research UK Cambridge Institute, University of Cambridge\nRoles: writing; conceptualisation; coding\nHugo Tavares  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing; conceptualisation; coding\nAndries van Tonder  \nAffiliation: Department of Veterinary Medicine, University of Cambridge\nRoles: writing",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "Citation",
    "text": "Citation\n\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in Manzano Garcia & Tavares (2023).”.\n\nYou can cite these materials as:\n\nManzano Garcia R, Tavares H, van Tonder A (2024) “cambiotraining/bioinformatics-software-pipelines: Managing Bioinformatics Software and Pipelines”, https://cambiotraining.github.io/bioinformatics-software-pipelines\n\nOr in BibTeX format:\n@Misc{,\n  author = {Manzano Garcia, Raquel and Tavares, Hugo and van Tonder, Andries},\n  title = {cambiotraining/bioinformatics-software-pipelines: Managing Bioinformatics Software and Pipelines},\n  month = {May},\n  year = {2024},\n  url = {https://cambiotraining.github.io/bioinformatics-software-pipelines},\n}",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nYou can download the data used in these materials as a zip file from dropbox.\nDownload",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#setup",
    "href": "setup.html#setup",
    "title": "Data & Setup",
    "section": "Setup",
    "text": "Setup\n\nConda\nWe recommend using the Conda package manager to install your software. In particular, the newest implementation called Mamba.\nTo install Mamba, run the following commands from the terminal:\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p $HOME/miniforge3\nrm Miniforge3-$(uname)-$(uname -m).sh\n$HOME/miniforge3/bin/mamba init\nRestart your terminal (or open a new one) and confirm that your shell now starts with the word (base). Then run the following commands:\nRestart your terminal (or open a new one) and confirm that your shell now starts with the word (base). Then run the following commands:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nconda config --set remote_read_timeout_secs 1000\n\n\nNextflow\nWe recommend that you install Nextflow within a conda environment. You can do that with the following command:\nmamba create -n nextflow -y nextflow\nWhen you want to use Nextflow make sure to activate this software environment by running mamba activate nextflow.\n\n\nSingularity\nWe recommend that you install Singularity and use the -profile singularity option when running Nextflow pipelines. On Ubuntu/WSL2, you can install Singularity using the following commands:\nsudo apt install -y runc cryptsetup-bin uidmap\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v4.0.2/singularity-ce_4.0.2-$(lsb_release -cs)_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb\nIf you have a different Linux distribution, you can find more detailed instructions on the Singularity documentation page.\nIf you have issues running Nextflow pipelines with Singularity, then you can follow the instructions below for Docker instead.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html",
    "href": "materials/01-package_managers.html",
    "title": "3  Package Managers",
    "section": "",
    "text": "3.1 What is a package manager?\nMost operating systems have package managers available, which allow the user to manage (install, remove, upgrade) their software easily. The package manager takes care of automatically downloading and installing the software we want, as well as any dependencies it requires.\nThere are many package managers available, some are specific to a given type of operating system, or specific to a programming language, while others are more generic. Each of these package managers will use their own repositories, meaning they have access to different sets of software (although there is often some overlap). Some examples include:\nSome programming languages also come with their own package managers. For example:\nIn many cases package managers can also install software directly from code repositories such as GitHub, adding further flexibility to how we manage our scientific software.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#what-is-a-package-manager",
    "href": "materials/01-package_managers.html#what-is-a-package-manager",
    "title": "3  Package Managers",
    "section": "",
    "text": "Diagram illustrating how package managers work. Image source.\n\n\n\n\napt is the default Linux package manager for Debian-derived distributions, such as the popular Ubuntu. It comes pre-installed and can be used to install system-level applications.\nhomebrew is a popular package manager for macOS, although it also works on Linux.\nconda/mamba is a package manager very popular in bioinformatics and data science communities, due to the repositories which give access to software used in these fields. It will be the main focus of this section.\n\n\n\nThe statistical software R has two main library repositories: CRAN and Bioconductor. These are installed from within the R console using the commands install.packages() and BiocManager::install(), respectively.\nThe programming laguage Python has a package manager called pip, which has access to the Python Package Index (PyPI) repository.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#condamamba",
    "href": "materials/01-package_managers.html#condamamba",
    "title": "3  Package Managers",
    "section": "3.2 Conda/Mamba",
    "text": "3.2 Conda/Mamba\nA popular package manager in data science, scientific computing and bioinformatics is Mamba, which is a successor to another package manager called Conda.\nConda was originally developed by Anaconda as a way to simplify the creation, distribution, and management of software environments containing different packages and dependencies. It is known for its cross-platform compatibility and relative ease of use (compared to compiling software and having the user manually install all software dependencies). Mamba is a more recent and high-performance alternative to Conda. While it maintains compatibility with Conda’s package and environment management capabilities, Mamba is designed for faster dependency resolution and installation, making it a better choice nowadays. Therefore, the rest of this section focuses on Mamba specifically.\nOne of the strengths of using Mamba to manage your software is that you can have different versions of your software installed alongside each other, organised in environments. Organising software packages into environments is extremely useful, as it allows to have a reproducible set of software versions that you can use and reuse in your projects.\nFor example, imagine you are working on two projects with different software requirements:\n\nProject A: requires Python 3.7, NumPy 1.15, and scikit-learn 0.20.\nProject B: requires Python 3.12, the latest version of NumPy, and TensorFlow 2.0.\n\nIf you don’t use environments, you would need to install and maintain these packages globally on your system. This can lead to several issues:\n\nVersion conflicts: different projects may require different versions of the same library. For example, Project A might not be compatible with the latest NumPy, while Project B needs it.\nDependency chaos: as your projects grow, you might install numerous packages, and they could interfere with each other, causing unexpected errors or instability.\nDifficulty collaborating: sharing your code with colleagues or collaborators becomes complex because they may have different versions of packages installed, leading to compatibility issues.\n\n\n\n\nIllustration of Conda/Mamba environments. Each environment is isolated from the others (effectively in its own folder), so different versions of the packages can be installed for distinct projects or parts of a long analysis pipeline.\n\n\nMamba allows you to create self-contained software environments for each project, addressing these issues:\n\nIsolation: you can create a separate environment for each project. This ensures that the dependencies for one project don’t affect another.\nSoftware versions: you can specify the exact versions of libraries and packages required for each project within its environment. This eliminates version conflicts and ensures reproducibility.\nEase of collaboration: sharing your code and environment file makes it easy for collaborators to replicate your environment and run your project without worrying about conflicts.\nSimplified maintenance: if you need to update a library for one project, it won’t impact others. You can manage environments separately, making maintenance more straightforward.\n\nAnother advantage of using Mamba is that the software is installed locally (by default in your home directory), without the need for admin (sudo) permissions.\n\n3.2.1 Installing software with Mamba\nYou can search for available packages from the anaconda.org website. Packages are organised into “channels”, which represent communities that develop and maintain the installation “recipes” for each software. The most popular channels for bioinformatics and data analysis are “bioconda” and “conda-forge”.\nThere are three main commands to use with Mamba:\n\nmamba create -n ENVIRONMENT-NAME: this command creates a new software environment, which can be named as you want. Usually people name their environments to either match the name of the main package they are installing there (e.g. an environment called pangolin if it’s to install the Pangolin software). Or, if you are installing several packages in the same environment, then you can name it as a topic (e.g. an environment called rnaseq if it contains several packages for RNA-seq data analysis).\nmamba install -n ENVIRONMENT-NAME  NAME-OF-PACKAGE: this command installs the desired package in the specified environment.\nmamba activate ENVIRONMENT-NAME: this command “activates” the environment, which means the software installed there becomes available from the terminal.\n\nLet’s see a concrete example. If we wanted to install packages for phylogenetic analysis, we could do:\n# create an environment named \"phylo\"\nmamba create -n phylo\n\n# install some software in that environment\nmamba install -n phylo iqtree==2.3.3 mafft==7.525\nIf we run the command:\nmamba env list\nWe will get a list of environments we created, and “phylo” should be listed there. If we want to use the software we installed in that environment, then we can activate it:\nmamba activate phylo\nAnd usually this changes your terminal to have the word (phylo) at the start of your prompt instead of (base).\n\n\n3.2.2 Environment files\nAlthough we can create and manage environments as shown above, it may sometimes be useful to specify an environment in a file. This is particularly useful if you want to document how your environment was created and if you want to recreate it somewhere else.\nEnvironments can be defined using a specification file in YAML format (a simple text format often used for configuration files). For example, our phylogenetics environment above could be specified as follows:\nname: phylo\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - iqtree==2.3.3\n  - mafft==7.525\nWe have included this example in the file demo/envs/phylo.yml. To create the environment from the file, we can use the command:\nmamba env create -f envs/phylo.yml\nIf you later decide to update the environment, either by adding a new software or by updating the software versions, you can run the command:\nmamba env update -f envs/phylo.yml\nYou can practice this in an exercise below.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#disadvantages-and-pitfalls",
    "href": "materials/01-package_managers.html#disadvantages-and-pitfalls",
    "title": "3  Package Managers",
    "section": "3.3 Disadvantages and pitfalls",
    "text": "3.3 Disadvantages and pitfalls\nOne thing to be very careful about is how Conda/Mamba manages the dependency graph of packages to install. If you don’t specify the version of the software you want, in theory Mamba will pick the latest version available on the channel. However, this is conditional on the other packages that are installed alongside it, as some versions may be incompatible with each other, it may downgrade some packages without you realising.\nTake this example, where we create a new environment called metagen with some software for metagenomic analysis:\nmamba create -n metagen multiqc bowtie2 metaphlan\nAt the time of writing, the latest version of metaphlan on anaconda.org is 4.1.0, however as we run this command we can see that Mamba is installing version 4.0.6.\nLet’s be more explicit and specify we want the latest versions available of all three packages (at the time of writing):\nmamba create -n metagen multiqc==1.21 bowtie2==2.5.3 metaphlan==4.1.0\nBy running this command, we get an error message informing us that Mamba could not find a fully compatible environment for all these three software versions:\nCould not solve for environment specs\nThe following packages are incompatible\n\n... followed by a ridiculously long message explanining the sofware incompatibilities ...\nHow would we solve this problem? One possibility is to install each software in a separate environment. The disadvantage is that you will need to run several mamba activate commands at every step of your analysis.\nAnother possibility is to find a compatible combination of package versions that is sufficient for your needs. For example, let’s say that metaphlan was the most critical software for which we needed to run the latest version. We could find what versions of the other two packages are compatible with it, by forcing its version, but not the version of the other two:\nmamba create -n metagen multiqc bowtie2 metaphlan==4.1.0\nRunning this command, we can see that we would get bowtie2==2.5.1 and multiqc==1.21. So, bowtie would be a slightly older version than currently available. But if we were happy with this choice, then we could proceed. For reproducibility, we could save all this information in a YAML file specifying our environment:\nname: metagen\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - bowtie==2.5.1\n  - multiqc==1.21\n  - metaphlan==4.1.0\n\n\n\n\n\n\nMixing package managers\n\n\n\nThere might be times when some packages/libraries are not available in a package manager. For example, it can be common to use conda/mamba but find a python library that is only available through pip. Unfortunately, this may cause issues in your environment as pip may change your conda-installed packages, which might break the conda environment. There are a few steps one can follow to avoid this pitfalls:\n\nStart from a new and clean environment. If the new environment break you can safely remove it and start over. You can create a new environment from pre-existing ones if necessary. We will see more of this later.\nInstall pip in your conda environment. This is important as the pip you have in your base environment is different from your new environment (will avoid conflicts).\nInstall any conda packages your need to get the environment ready and leave the pip install for last. Avoid switching between package managers. Start with one and finish with the other one so reversing or fixing conflicts is easier.\n\nYou can find a (checklist)[https://www.anaconda.com/blog/using-pip-in-a-conda-environment] in the anaconda webpage for good practice.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#exercises",
    "href": "materials/01-package_managers.html#exercises",
    "title": "3  Package Managers",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\n\n\n\n\n\nCreating a new Mamba environment\n\n\n\n\n\n\n\nGo to the demo directory, where you will find some FASTQ files in the reads folder. The objective in this exercise is to setup a software environment to run a standard quality control software on these sequencing reads.\n\nUse a text editor to create a Conda/Mamba environment file called envs/qc.yml. This file should specify:\n\nEnvironment name: qc\nChannels: conda-forge, bioconda\nPackages: FastQC v0.12.1 and MultiQC v1.21 (check available packages at anaconda.org).\n\nUsing mamba build the environment from your created file.\nActivate your new environment and run the QC script provided: bash scripts/01-qc.sh (you can look inside the script to see what it is doing).\nCheck if you obtained the final output file in results/qc/multiqc_report.html.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe can see how to specify an environment file manually on the Conda documentation page. Following those instructions, we have created the following file and saved it as envs/qc.yml:\nname: qc\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - fastqc==0.12.1\n  - multiqc==1.21\nWe then created our environment with the command:\nmamba env create -f envs/qc.yml\nWe then activate our environment:\nmamba activate qc\nAnd finally ran the script provided:\nbash scripts/01-qc.sh\nWe can see the script ran successfully by looking at the output directory results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate a Mamba environment\n\n\n\n\n\n\n\nGoing back to the envs/phylo.yml environment (in the demo folder), update the environment to include a software to for dating phylogenetic trees called TreeTime.\n\nGo to anaconda.org to see what is the latest version available and from which channel.\nUpdate the YAML environment file to include it.\nUpdate the environment.\nCheck if the software was installed successfully by running treetime --version.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe can see the software is available from https://anaconda.org/bioconda/treetime, provided from the bioconda channel. The latest version at the time of writing is 0.11.3, so it is the one we demonstrate below.\nUsing a text editor of our choice, we update our YAML file:\nname: phylo\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - iqtree==2.3.3\n  - mafft==7.525\n  - treetime==0.11.3\nAfter saving the changes, we update our environment:\nmamba env update -f envs/phylo.yml\nOnce the update runs successfully, we activate the environment first with mamba activate phylo and then test our software:\ntreetime --version\ntreetime 0.11.3\nThe command runs successfully, with the expected version printed, indicating it is successfully installed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nTODO: example of environment with conflicts?",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#summary",
    "href": "materials/01-package_managers.html#summary",
    "title": "3  Package Managers",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA package manager automates the process of installing, upgrading, configuring, and managing software packages, including their dependencies.\nExamples of package managers are pip (Python), apt (Debian/Ubuntu) and conda/mamba (generic).\nDependency conflicts, which often arise in complex bioinformatic workflows, can be resolved by managing software in isolated environments.\nConda/Mamba simplify these tasks by managing dependencies, creating isolated environments, and ensuring reproducible setups across different systems.\nKey Mamba commands include:\n\nmamba create --name ENVIRONMENT-NAME to create a new environment.\nmamba install -n ENVIRONMENT-NAME  NAME-OF-PACKAGE to install a package inside that environment.\nmamba activate ENVIRONMENT-NAME to make the software from that environment available.\nmamba env create -f ENVIRONMENT-YAML-SPECIFICATION to create an environment from a YAML file (recommended for reproducibility).\nmamba env update -f ENVIRONMENT-YAML-SPECIFICATION to update an environment from a YAML file (recommended for reproducibility).\n\nRecognise some of limitations of Mamba as a package manager and how to avoid common pitfalls.\nThere are some disadvantages/limitations of Mamba as a package manager:\n\nDependencies aren’t always respected.\nSoftware versions are sometimes downgraded without explicit warning.\nIt can be slow at resolving very complex environments.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html",
    "href": "materials/02-containers.html",
    "title": "4  Container Virtualisation",
    "section": "",
    "text": "4.1 Overview\nSoftware containerisation is a way to package software and its dependencies in a single file. A software container can be thought of as a small virtual machine, with everything needed to run that software stored inside that file. Software containers are self-contained, meaning that they are isolated from the host system. This ensures reproducibility, addressing the issue of incompatible dependencies between tools (similarly to Mamba environments). They can run on a local computer or on a high-performance computing cluster, producing the same result. The same analysis can be run on different systems ensuring consistency and reproducibility.\nFor these reasons, software containerisation solutions, such as Docker and Singularity, are widely used in bioinformatics. While these two container software solutions share many similarities, we will focus our attention on Singularity, as it is more widely used in HPC cluster systems (but it can also be used on a regular computer). However, it’s worth keeping in mind that images created with Docker can be compatible with Singularity and vice versa.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#overview",
    "href": "materials/02-containers.html#overview",
    "title": "4  Container Virtualisation",
    "section": "",
    "text": "Docker vs singularity\n\n\n\n\n\nThere are some key differences between Docker containers and Singularity containers. The most important being the necessary permission level of the containers. Docker containers run as root (admin) by default, which means that they have full access to the host system. While this can be advantageous in some cases, it can also pose security risks, particularly in multi-user environments. Singularity, on the other hand, runs containers as non-root users by default, which can improve security and prevent unauthorized access to the host system. Singularity is specifically designed for use in HPC environments and can run on a wide variety of platforms and systems without root access.\nTL;TR:\n\nDocker is well-suited for building and distributing software across different platforms and operating systems.\nSingularity is specifically designed for use in HPC environments and can provide improved security and performance in those settings.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#singularity-images",
    "href": "materials/02-containers.html#singularity-images",
    "title": "4  Container Virtualisation",
    "section": "4.2 Singularity images",
    "text": "4.2 Singularity images\nContainers are constructed from images - these are executable files that generate the container. To help understand this, consider images like a sheet of music and containers as the actual music you hear. The sheet music is portable and can be performed on various instruments, yet the melody remains consistent regardless of where it’s played.\nAlthough you can build your own Singularity images, for many popular software there are already pre-built images available from public repositories. Some popular ones are:\n\nGalaxy Project: an open-source platform for data analysis available to researchers and the community. They provide the galaxy depot from where you can navigate to their singularity images and choose the one that you need for your analysis.\nSylabs: Singularity Container Services from the Singularity developers.\ndockerhub: a more generic repository of Docker images, which can also be used with Singularity.\n\nFor example, let’s consider the SeqKit program, which is a toolkit for manipulating FASTA/Q files. If we search on those websites, we will see this software is available on all of them. In this case, the version on Sylabs (here) is older than the one on the Galaxy server (at the time of writing we have 2.8.0 available).\nTherefore, let’s consider the file on the Galaxy server. First, go to depot.galaxyproject.org and search for the software of interest (use Ctrl + F to find the text of interest). When you find the software and version of interest, right-click the file and click “Copy Link”. Then use that link with the singularity pull command:\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/seqkit-2.8.0.sif https://depot.galaxyproject.org/singularity/seqkit%3A2.8.0--h9ee0642_0\nHere, we are saving the image file as seqkit-2.8.0.sif (.sif is the standard extension for singularity images). Once we have this image available, we are ready to run the software, which will see in practice with the exercise below.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#executing-commands",
    "href": "materials/02-containers.html#executing-commands",
    "title": "4  Container Virtualisation",
    "section": "4.3 Executing commands",
    "text": "4.3 Executing commands\nTo execute a command inside the image container, we can use the singularity run command. For example, let’s run the command seqkit --help to look at the help documentation of the SeqKit software:\nsingularity run images/seqkit-2.8.0.sif seqkit --help\nAnd this should print the help of the program. Note that SeqKit is not installed on your system, rather the command is avaible inside the container, as it was pre-installed in its image file.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#filesystem-binding",
    "href": "materials/02-containers.html#filesystem-binding",
    "title": "4  Container Virtualisation",
    "section": "4.4 Filesystem binding",
    "text": "4.4 Filesystem binding\nOne important thing to note is that, by default, not your entire filesystem is visible to the container. You have your home directory mounted on the image by default, but nothing else.\nIf you want to operate on files that are on a different file partition, you need to bind them to the container. This is done using the --bind option, which requires a full path to the directory you want to make available on the image.\nFor example, say a user called robin was working on a directory called /scratch/robin/awesomeproject. Because this directory is not in the default home (/home/robin), it will not be available on the container by default. The user could make it available as the default directory by running:\nsingularity run --bind /scratch/robin/awesomeproject images/seqkit-2.8.0.sif seqkit --help\nSee more information about this in the Singularity documentation.\n\n\n\n\n\n\nSingularity on HPC systems\n\n\n\nTypically, Singularity is pre-installed on HPC servers by the system administrators, and we recommend that you use the version installed by your system admins. This is because they will have often already set specific bindings to the filesystem, meaning you don’t need to do this yourself.\n\n\n\n4.4.1 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nTo illustrate the use of Singularity, we will use the seqkit software to extract some basic statistics from the sequencing files in the demo/reads directory. If you haven’t done so already, first download the container with the commands shown above.\nThe way to check a command within a singularity container is:\nsingularity run images/seqkit-2.8.0.sif seqkit --help\n\nWrite a command to run the command seqkit stats reads/*.fastq.gz using the singularity container we downloaded earlier.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe Singularity command is:\nsingularity run images/seqkit-2.8.0.sif seqkit stats reads/*.fastq.gz\nIf we run this, it produces an output like this:\nfile                                     format  type   num_seqs      sum_len  min_len  avg_len  max_len\nreads/SRR7657872_1.downsampled.fastq.gz  FASTQ   DNA   1,465,993  219,898,950      150      150      150\nreads/SRR7657872_2.downsampled.fastq.gz  FASTQ   DNA   1,465,993  219,898,950      150      150      150\nreads/SRR7657874_1.downsampled.fastq.gz  FASTQ   DNA   1,379,595  206,939,250      150      150      150\nreads/SRR7657874_2.downsampled.fastq.gz  FASTQ   DNA   1,379,595  206,939,250      150      150      150\nreads/SRR7657876_1.downsampled.fastq.gz  FASTQ   DNA   1,555,049  233,257,350      150      150      150\nreads/SRR7657876_2.downsampled.fastq.gz  FASTQ   DNA   1,555,049  233,257,350      150      150      150\nreads/SRR7657877_1.downsampled.fastq.gz  FASTQ   DNA   1,663,432  249,514,800      150      150      150\nreads/SRR7657877_2.downsampled.fastq.gz  FASTQ   DNA   1,663,432  249,514,800      150      150      150",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#summary",
    "href": "materials/02-containers.html#summary",
    "title": "4  Container Virtualisation",
    "section": "4.5 Summary",
    "text": "4.5 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nSoftware containerisation solutions such as Docker and Singularity achieve isolation and consistency in running software. This is because they encapsulate applications with their dependencies in a file (known as an image), ensuring consistent performance across different environments.\nThe computer science and bioinformatics community has built many images for commonly used software, including the galaxy depot, Sylabs and dockerhub.\nTo download an image from the online repositories we can use the command singularity pull &lt;URL TO IMAGE&gt;\nTo run a command using an existing image, we use the command: singularity run &lt;PATH TO IMAGE&gt; &lt;YOUR COMMAND&gt;\nWe can mount specific directories in our filesystem to the Singularity container using the --bind option with a full path to the directory we want to make available.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html",
    "href": "materials/03-nfcore.html",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "5.1 Overview\nBioinformatic analyses always involve multiple steps where data is gathered, cleaned and integrated to give a final set of processed files of interest to the user. These sequences of steps are called a workflow or pipeline. As analyses become more complex, pipelines may include the use of many different software tools, each requiring a specific set of inputs and options to be defined. Furthermore, as we want to chain multiple tools together, the inputs of one tool may be the output of another, which can become challenging to manage.\nAlthough it is possible to code such workflows using shell scripts, these often don’t scale well across different users and compute setups. To overcome these limitations, dedicated workflow/pipeline management software packages have been developed to help standardise pipelines and make it easier for the user to process their data. These dedicated packages are designed to streamline and automate the process of coordinating complex sequences of tasks and data processing (for instance an RNA-seq analysis). In this way, researchers can focus on their scientific questions instead of the nitty-gritty of data processing.\nHere are some of the key advantages of using a standardised workflow for our analysis:",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#overview",
    "href": "materials/03-nfcore.html#overview",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "A) Workflow illustration, showing ready-for-running tasks highlighted in green, indicating all necessary input files are available. The initial red task produces a temporary file, potentially removable once the blue tasks complete. Workflow management systems ensure tasks are run in an optimal and automated manner. For example, in B) there is suboptimal scheduling of tasks, as only one blue task is scheduled the temporary file cannot be removed. Conversely, in C) we have optimal scheduling, since the three blue tasks are scheduled first enabling deletion of the temporary file after their completion. Diagram taken from Mölder et al. 2021.\n\n\n\n\nFewer errors - because the workflow automates the process of managing input/output files, there are less chances for errors or bugs in the code to occur.\nConsistency and reproducibility - analysis ran by different people should result in the same output, regardless of their computational setup.\nSoftware installation - all software dependencies are automatically installed for the user using solutions such as Conda, Docker and Singularity (more about these in a later section of the course).\nScalability - workflows can run on a local desktop or scale up to run on high performance compute clusters.\nCheckpoint and resume - if a workflow fails in one of the tasks, it can be resumed at a later time.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#nextflow-and-snakemake",
    "href": "materials/03-nfcore.html#nextflow-and-snakemake",
    "title": "5  Automated Workflows",
    "section": "5.2 Nextflow and Snakemake",
    "text": "5.2 Nextflow and Snakemake\nTwo of the most popular workflow software packages are Snakemake and Nextflow. We will not cover how to develop workflows with these packages, but rather how to use existing workflows developed by the community.1 Both Snakemake and Nextflow offer similar functionality and can work on different computer systems, from personal laptops to large cloud-based platforms, making them very versatile. One of the main noticeable difference to those developing pipelines with these tools is that Snakemake syntax is based on Python, whereas Nextflow is based on groovy. The choice between one of the other is really down to individual preference.\n\nAnother important aspect of these projects are the workflows and modules provided by the community:\n\nnf-core: a community project where scientists contribute ready-to-use, high-quality analysis pipelines. This means you don’t have to start from scratch if someone else has already created a workflow for a similar analysis. It’s all about making data analysis more accessible, standardized, and reproducible across the globe​​​​.\nSnakemake workflow catalog: a searcheable catalog of workflows developed by the community, with instructions and details on how to use them. Although there is some standardisation of these pipelines, they are not as well curated as the ones from nf-core.\n\nThese materials will focus on Nextflow, due to the standarised and ready-to-use pipelines available through nf-core.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#nextflow-command-line-interface",
    "href": "materials/03-nfcore.html#nextflow-command-line-interface",
    "title": "5  Automated Workflows",
    "section": "5.3 Nextflow command line interface:",
    "text": "5.3 Nextflow command line interface:\nNextflow has an array of subcommands for the command line interface to manage and execute pipelines. To see all options you can simply run nextflow -h and a list of available top-level options will appear in your terminal’. here we highlight the three we will be using:\n\nnextflow run [options] [pipeline]: will execute a nextflow pipeline\nnextflow log: will print the execution history and log information of a pipeline\nnextflow clean: will clen up cache and work directories.\n\nThe command nextflow run has some useful options:\n\n-profile: defines which configuration profile(s) to use.\n-resume: restarts the pipeline where it last stopped (using cached results).\n-work-dir: defines the directory where intermediate result files (cache) are stored.\n\nWe detail each of these below.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#configuration-profile",
    "href": "materials/03-nfcore.html#configuration-profile",
    "title": "5  Automated Workflows",
    "section": "5.4 Configuration profile",
    "text": "5.4 Configuration profile\nThere are several ways to configure how our Nextflow workflow runs. All nf-core workflows come with some default profiles that we can choose from:\n\nsingularity uses Singularity images for software management. This is the recommended (including on HPC systems).\ndocker uses Docker images for software management.\nmamba uses Mamba for software management. This is not recommended as it is known to be slow and buggy.\n\nMore details about these in the nf-core documentation.\nTo use one of these profiles we use the option -profile followed by the name of the configuration profile we wish to use. For example, -profile singularity will use Singularity to manage and run the software.\nSometimes you may want to use custom profiles or the pipeline you are using is not from the nf-core community. In that case, you can define your own profile. The easiest may be to look at one of the nf-core configuration files and set your own based on that. For example, to set a profile for Singularity, we create a file with the following:\nprofiles {\n  singularity {\n    singularity.enabled    = true\n  }\n}\nLet’s say we saved this file as nextflow.config. We can then use this profile by running our pipeline with the options -profile singularity -c nextflow.config. You can also specify more than one configuration file in your command as -c myconfig1 -c myconfig2.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#cache-directory",
    "href": "materials/03-nfcore.html#cache-directory",
    "title": "5  Automated Workflows",
    "section": "5.5 Cache directory",
    "text": "5.5 Cache directory\nWhen a Nextflow pipeline runs it creates (by default) a work directory when you execute the pipeline for the first time. The work directory stores a variety of intermediate files used during the pipeline run, called a cache. The storage of these intermediate files is very important, as it allows the pipeline to resume from a previous state, in case it ran with errors and failed half-way through (more on this below).\nEach task from the pipeline (e.g. a bash command can be considered a task) will have a unique directory name within work. When a task is created, Nextflow stages the task input files, script, and other helper files into the task directory. The task writes any output files to this directory during its execution, and Nextflow uses these output files for downstream tasks and/or publishing. Publishing is when the output of a task is being saved to the output directory specified by the user.\nThe -work-dir option can be used to change the name of the cache directory from the default work. This default directory name is fine (and most people just use that), but you may sometimes want to define a different one. For example, if you coincidentally already have a directory called “work” in your project, or if you want to use a separate storage partition to save the intermediate files.\nRegardless, it is important to remember that your final results are not stored in the work directory. They are saved to the output directory you define when you run the pipeline. Therefore, after successfully finishing your pipeline you can safely remove the work directory. This is important to save disk space and you should make sure to do it regularly.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#checkpoint-and-resume",
    "href": "materials/03-nfcore.html#checkpoint-and-resume",
    "title": "5  Automated Workflows",
    "section": "5.6 Checkpoint-and-resume",
    "text": "5.6 Checkpoint-and-resume\nBecause Nextflow is keeping track of all the intermediate files it generates, it can re-run the pipeline from a previous step, if it failed half-way through. This is an extremely useful feature of workflow management systems and it can save a lot of compute time, in case a pipeline failed.\nAll you have to do is use the option -resume when launching the pipeline and it will always resume where it left off. Note that, if you remove the work cache directory detailed above, then the pipeline will have to start from the beginning, as it doesn’t have any intermediate files saved to resume from.\nMore information about this feature can be found in the Nextflow documentation.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#samplesheet",
    "href": "materials/03-nfcore.html#samplesheet",
    "title": "5  Automated Workflows",
    "section": "5.7 Samplesheet",
    "text": "5.7 Samplesheet\nMost nf-core pipelines use a CSV file as their main input file. These CSV file is often referred to as the “sampleshet”, as it contains information about each sample to be processed.\nAlthough there is no universal format for this CSV file, most pipelines accept at least 3 columns:\n\nsample: a name for the sample, which can be anything of our choice.\nfastq_1: the path to the respective read 1 FASTQ file.\nfastq_2: the path to the respective read 2 FASTQ file. This value is optional and, if missing it is assumed the sample is from single-end sequencing.\n\nThese are only the most basic columns, however many other columns are often accepted, depending on the specific pipeline being used. The details for the input CSV samplesheet are usually given in the “Usage” tab of the documentation.\nAs FASTQ files are often named using very long identifiers, it’s a good idea to use some command line tricks to save typing and avoid typos. For example, we can create a first version of our file by listing all read 1 files and saving it into a file:\nls reads/*_R1.fq.gz &gt; samplesheet.csv\nThis will give us a good start to create the samplesheet. We can then open this file in a spreadsheet software such as Excel and create the remaining columns. We can copy-paste the file paths and use the “find and replace” feature to replace “R1” with “R2”. This way we save a lot of time of typing but also reduce the risk of having typos in our file paths.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#demo-nf-core-pipeline",
    "href": "materials/03-nfcore.html#demo-nf-core-pipeline",
    "title": "5  Automated Workflows",
    "section": "5.8 Demo nf-core pipeline",
    "text": "5.8 Demo nf-core pipeline\nTo demonstrate the use of standard nf-core pipelines, we will use the aptly named nf-core/demo pipeline. This workflow takes a set of FASTQ files as input, runs them through a simple QC step and outputs processed files as well as a MultiQC quality report.\n\nWe will run this workflow on a set of files found in the demo directory. Looking at the documentation, we are given an example of the samplesheet CSV file.\nThis is how the samplesheet looks like for our samples:\nsample,fastq_1,fastq_2\ndrug_rep2,reads/SRR7657872_1.downsampled.fastq.gz,reads/SRR7657872_2.downsampled.fastq.gz\ndrug_rep1,reads/SRR7657874_1.downsampled.fastq.gz,reads/SRR7657874_2.downsampled.fastq.gz\ncontrol_rep2,reads/SRR7657876_1.downsampled.fastq.gz,reads/SRR7657876_2.downsampled.fastq.gz\ncontrol_rep1,reads/SRR7657877_1.downsampled.fastq.gz,reads/SRR7657877_2.downsampled.fastq.gz\nWe have named our samples using informative names of our choice, and indicate the path to the respective FASTQ input files. We can then run our workflow as follows:\nnextflow run -profile \"singularity\" -revision \"dev\" nf-core/demo \\\n  --max_memory \"12GB\" --max_cpus 8 \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/qc\" \\\n  --fasta \"genome/Mus_musculus.GRCm38.dna_sm.chr14.fa.gz\"\nIn this case we used the following options:\n\n-profile \"singularity\" indicates we want to use Singularity to manage the software. Nextflow will automatically download containers for each step of the pipeline.\n-revision \"dev\" means we are running the development version of the pipeline. It’s a good idea to define the specific version of the pipeline you run, so you can reproduce the results in the future, in case the pipeline changes. This demo pipeline only has a development version, but usually versions are numbered (some examples will be shown in the exercises).\n--input is the samplesheet CSV for this pipeline, which we prepared beforehand using a spreadsheet program such as Excel.\n--outdir is the name of the output directory for our results.\n--fasta is the reference genome to be used by the pipeline.\n\nWhen the pipeline starts running, we are given information about its progress, for example:\nexecutor &gt;  local (3)\n[-        ] process &gt; NFCORE_DEMO:DEMO:FASTQC            -\n[72/e1a45a] process &gt; NFCORE_DEMO:DEMO:FASTP (drug_rep1) [ 50%] 2 of 4\n[-        ] process &gt; NFCORE_DEMO:DEMO:MULTIQC           -\nYou will also notice that a new directory called work is created. As mentioned above, this is the cache directory, which stores intermediate files and allows the workflow to resume if it fails half-way through (using the -resume option).\nOnce the pipeline completes (hopefully successfully), we are given a message:\n\nIf we are happy with our results, we can clean the work cache directory to save space:\nnextflow clean\n\n\n\n\n\n\nReference genomes\n\n\n\nTODO: add note about reference genomes and why using igenomes is not recommended.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#troubleshooting",
    "href": "materials/03-nfcore.html#troubleshooting",
    "title": "5  Automated Workflows",
    "section": "5.9 Troubleshooting",
    "text": "5.9 Troubleshooting\nInevitably workflows may fail, which could be due to several reasons. For example, an error in our command, a mis-formatted samplesheet, missing input files or sometimes even a bug in the pipeline.\nWhen an error occurs, the nextflow command terminates and an error message is printed on the screen (usually in bright red!). The error messages can be quite long and feel difficult to interpret, but often only a small part of the message is relevant, so read it careful to see if you can spot what the problem is.\nFor example, we previously got the following error when running the nf-core/demo pipeline. Can you see what the problem was?\n\n-[nf-core/demo] Pipeline completed with errors-\n\nERROR ~ Error executing process &gt; 'NFCORE_DEMO:DEMO:FASTP (drug_rep1)'\n\nCaused by:\n  Process requirement exceeds available memory -- req: 36 GB; avail: 23.5 GB\n\nCommand executed:\n\n  [ ! -f  drug_rep1_1.fastq.gz ] && ln -sf SRR7657874_1.downsampled.fastq.gz drug_rep1_1.fastq.gz\n  [ ! -f  drug_rep1_2.fastq.gz ] && ln -sf SRR7657874_2.downsampled.fastq.gz drug_rep1_2.fastq.gz\n  fastp \\\n      --in1 drug_rep1_1.fastq.gz \\\n      --in2 drug_rep1_2.fastq.gz \\\n      --out1 drug_rep1_1.fastp.fastq.gz \\\n      --out2 drug_rep1_2.fastp.fastq.gz \\\n      --json drug_rep1.fastp.json \\\n      --html drug_rep1.fastp.html \\\n       \\\n       \\\n       \\\n      --thread 6 \\\n      --detect_adapter_for_pe \\\n       \\\n      2&gt; &gt;(tee drug_rep1.fastp.log &gt;&2)\n  \n  cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n  \"NFCORE_DEMO:DEMO:FASTP\":\n      fastp: $(fastp --version 2&gt;&1 | sed -e \"s/fastp //g\")\n  END_VERSIONS\n\n\n\n\n\nClick here for the answer\n\nAlthough this is a long message, the cause of the error itself is at the top where we are told “Process requirement exceeds available memory – req: 36 GB; avail: 23.5 GB”.\nThis means a step of the pipeline must have requested 36GB by default, but we only had 23.5GB on the computer used to run it. In this case, we can add two options to our command that every nf-core pipeline has: --max_memory 20GB --max_cpus 8.\nNote that in real-world cases, where you are likely running these workflows on a HPC you don’t need to worry about this. We will learn more about running workflows on HPC in the next section.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#exercises",
    "href": "materials/03-nfcore.html#exercises",
    "title": "5  Automated Workflows",
    "section": "5.10 Exercises",
    "text": "5.10 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nIn this exercise, you will explore one (or more, if you have time and interest) of the pipelines from the nf-core community, tailored to different areas of genomic analysis. Start with the version of the exercise that aligns best with your data and interests.\nCarefully read the respective nf-core pipeline documentation to understand the required format for the samplesheet and any specific parameters needed for running the pipeline. Once your samplesheet is ready, launch the pipeline and watch how Nextflow orchestrates the different steps of the analysis.\nIn each case, remember to use the -r option to specify the version of the workflow to be used (use the latest available on the respective documentation page) and tell Nextflow to use singularity to manage the software.\nNote that all of these datasets are downsampled to be small, so they run quickly. They do not represent best practices in experimental design.\n\nRNA-seqChIP-seqVirus genomes - IlluminaVirus genomes - ONTVariant calling\n\n\nTranscriptome data processing using nf-core/rnaseq. Go into the rnaseq directory for this version of the exercise.\n\nDocumentation at nf-co.re/rnaseq/.\nInput FASTQ files in reads/.\nReference genome in genome/.\nSample metadata in sample_info.tsv (tab-delimited).\n\n\n\nChromatin immunoprecipitation sequencing analysis using nf-core/chipseq. Go into the chipseq directory for this version of the exercise.\n\nDocumentation at nf-co.re/chipseq/.\nInput FASTQ files in reads/.\nReference genome in genome/\nSample metadata in sample_info.tsv (tab-delimited).\n\n\n\nAnalysis of viral genomes using nf-core/viralrecon. Go into the virus_illumina directory for this version of the exercise.\n\nDocumentation at nf-co.re/viralrecon/.\nInput FASTQ files in reads/.\nFASTA file for the reference genome, BED file for primer locations and GFF file with gene annotations in genome/ (see if you can find the pipeline parameters for each of these files in the documentation).\nSample metadata in sample_info.tsv (tab-delimited).\n\n\n\nAnalysis of viral genomes using nf-core/viralrecon. Go into the virus_ont directory for this version of the exercise.\n\nDocumentation at nf-co.re/viralrecon/.\nInput barcode directories with FASTQ files in fast_pass/.\nFASTA file for the reference genome, BED file for primer locations and GFF file with gene annotations in genome/ (see if you can find the pipeline parameters for each of these files in the documentation).\nSample metadata in sample_info.tsv (tab-delimited).\n\n\n\nIdentifying genetic variants using nf-core/sarek. Go into the variants directory for this version of the exercise.\n\nDocumentation at nf-co.re/sarek/.\nInput FASTQ files in reads/.\nFASTA file for the reference genome in genome/.\nSample metadata in sample_info.tsv (tab-delimited).\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nTODO",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#summary",
    "href": "materials/03-nfcore.html#summary",
    "title": "5  Automated Workflows",
    "section": "5.11 Summary",
    "text": "5.11 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWfMS define, automate and monitor the execution of a series of tasks in a specific order. They improve efficiency, reduce errors, can be easily scaled (from a local computer to a HPC cluster) and increase reproducibility.\nPopular WfMS in bioinformatics include Nextflow and Snakemake. Both of these projects have associated community-maintained workflows, with excellent documentation for their use: nf-core and the snakemake workflow catalog.\nTODO: finish key points",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#footnotes",
    "href": "materials/03-nfcore.html#footnotes",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "To learn how to build your own pipelines, there are many tutorials available on training.nextflow.io, including how to build a simple RNA-Seq workflow. Snakemake also provides an excellent tutorial covering both basic and advanced features to build custom pipelines.↩︎",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html",
    "href": "materials/04-nextflow_hpc.html",
    "title": "6  Nextflow on HPC",
    "section": "",
    "text": "6.1 HPC configuration\nTo run on a HPC, we can specify a profile specific to our HPC. For example, to work at the Cambridge University HPC we created the following configuration file:",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#hpc-configuration",
    "href": "materials/04-nextflow_hpc.html#hpc-configuration",
    "title": "6  Nextflow on HPC",
    "section": "",
    "text": "process {\n  executor = 'slurm'\n  \n  // generic option\n  clusterOptions = '--account LEYSER-SL2-CPU --partition icelake'\n\n  // Settings below are for CSD3 nodes detailed at\n  //   https://docs.hpc.cam.ac.uk/hpc/index.html\n  // Current resources (Jun 2023):\n  //   icelake: 76 CPUs; 3380 MiB per cpu; 6760 MiB per cpu (himem)\n  //   cclake: 56 CPUs; 3420 MiB per cpu; 6840 MiB per cpu (himem)\n  // The values used below were chosen to be multiples of these resources\n  // assuming a maximum of 2 retries\n\n  // Using himem partition to ensure enough memory for single-CPU jobs\n  withLabel:process_single {\n      cpus   = { check_max( 1                  , 'cpus'    ) }\n      memory = { check_max( 6800MB * task.attempt, 'memory'  ) }\n      time   = { check_max( 4.h  * task.attempt, 'time'    ) }\n      clusterOptions = '--account LEYSER-SL2-CPU --partition cclake-himem'\n  }\n  // 4 CPUs + 13GB RAM\n  withLabel:process_low {\n      cpus   = { check_max( 4     * task.attempt, 'cpus'    ) }\n      memory = { check_max( 13.GB * task.attempt, 'memory'  ) }\n      time   = { check_max( 4.h   * task.attempt, 'time'    ) }\n      clusterOptions = '--account LEYSER-SL2-CPU --partition cclake'\n  }\n  // 8 CPUs + 27GB RAM\n  withLabel:process_medium {\n      cpus   = { check_max( 8     * task.attempt, 'cpus'    ) }\n      memory = { check_max( 27.GB * task.attempt, 'memory'  ) }\n      time   = { check_max( 8.h   * task.attempt, 'time'    ) }\n      clusterOptions = '--account LEYSER-SL2-CPU --partition cclake'\n  }\n  // 12 CPUs + 40GB RAM\n  withLabel:process_high {\n      cpus   = { check_max( 12    * task.attempt, 'cpus'    ) }\n      memory = { check_max( 40.GB * task.attempt, 'memory')}\n      time   = { check_max( 8.h  * task.attempt, 'time'    ) }\n      clusterOptions = '--account LEYSER-SL2-CPU --partition cclake'\n  }\n  // Going by chunks of 12h (2 retries should bring it to max of 36h)\n  withLabel:process_long {\n      time   = { check_max( 12.h  * task.attempt, 'time'    ) }\n  }\n  // A multiple of 3 should bring it to max resources on cclake-himem\n  withLabel:process_high_memory {\n      cpus   = { check_max( 18     * task.attempt, 'cpus'    ) }\n      memory = { check_max( 127.GB * task.attempt, 'memory' ) }\n      clusterOptions = '--account LEYSER-SL2-CPU --partition cclake-himem'\n  }\n  withLabel:error_ignore {\n      errorStrategy = 'ignore'\n  }\n  withLabel:error_retry {\n      errorStrategy = 'retry'\n      maxRetries    = '2'\n  }\n}\n\nparams {\n  max_memory = '327.GB' // for cclake-himem\n  max_cpus = '56'       // for cclake nodes\n  max_time = '36.h'     // for SL2 service level\n}\n\nexecutor {\n  queueSize         = '2000'\n  pollInterval      = '3 min'\n  queueStatInterval = '5 min'\n  submitRateLimit   = '50sec'\n  exitReadTimeout   = '5 min'\n}\n\nsingularity {\n  singularity.enabled = 'true'\n  pullTimeout = '1 h'\n  cacheDir = '/home/hm533/rds/hpc-work/nextflow-singularity-cache'\n}",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#exercises",
    "href": "materials/04-nextflow_hpc.html#exercises",
    "title": "6  Nextflow on HPC",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\n\nCopy the configuration file for our “HPC” and modified according to exercise (add some FIX-ME?).\nSingularity cache in custom directory.\nRe-run the pipeline and see if it is submitting jobs to the scheduler as expected.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#summary",
    "href": "materials/04-nextflow_hpc.html#summary",
    "title": "6  Nextflow on HPC",
    "section": "6.3 Summary",
    "text": "6.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWfMS define, automate and monitor the execution of a series of tasks in a specific order. They improve efficiency, reduce errors, can be easily scaled (from a local computer to a HPC cluster) and increase reproducibility.\nPopular WfMS in bioinformatics include Nextflow and Snakemake. Both of these projects have associated community-maintained workflows, with excellent documentation for their use: nf-core and the snakemake workflow catalog.\nTODO: finish key points",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  }
]